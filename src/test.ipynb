{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/medhaaga/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def data_collate_fn(dataset_samples_list):\n",
    "    arr = np.array(dataset_samples_list)\n",
    "    inputs = tokenizer(text=arr.tolist(), padding='max_length', max_length=30, return_tensors='pt')\n",
    "    return inputs\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, src, tokenizer):\n",
    "        self.src = src\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src[idx]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = tokenizer.vocab_size # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    epochs = 1\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optim.zero_grad()\n",
    "            input = batch['input_ids'].clone()\n",
    "            src_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n",
    "            rand_value = torch.rand(batch.input_ids.shape)\n",
    "            rand_mask = (rand_value < 0.15) * (input != 101) * (input != 102) * (input != 0)\n",
    "            mask_idx = (rand_mask.flatten() == True).nonzero().view(-1)\n",
    "            input = input.flatten()\n",
    "            input[mask_idx] = 103\n",
    "            input = input.view(batch['input_ids'].size())\n",
    "            out = model(input.to(device), src_mask.to(device))\n",
    "            loss = criterion(out.view(-1, ntokens), batch['input_ids'].view(-1).to(device))\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    \n",
    "        if (epoch+1)%40==0 or epoch==0:\n",
    "            print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))\n",
    "        return mask_idx\n",
    "\n",
    "\n",
    "def predict(model, input):\n",
    "    model.eval()\n",
    "    src_mask = model.generate_square_subsequent_mask(input.size(1))\n",
    "    out = model(input.to(device), src_mask.to(device))\n",
    "    out = out.topk(1).indices.view(-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Don't speak ill of others.\",\n",
    "\"To speak ill of others is a great crime.\",\n",
    "\"Rather rectify yourself through self-criticism.\",\n",
    "\"In this way, if you rectify yourself, others will follow you.\",\n",
    "\"To speak ill of others gives rise to more problems.\",\n",
    "\"This does not do any good to society.\",\n",
    "\"More than 80 percent people of our country live in villages.\",\n",
    "\"Most of them are poor and illiterate.\",\n",
    "\"Illiteracy is one of the causes of their poverty.\",\n",
    "\"Many of the villagers are landless cultivators.\",\n",
    "\"They cultivate the lands of other people throughout the year.\",\n",
    "\"They get a very small portion of the crops.\",\n",
    "\"They provide all of us with food.\",\n",
    "\"But in want they only starve.\",\n",
    "\"They suffer most.\",\n",
    "\"The situation needs to be changed.\",\n",
    "\"We live in the age of science.\",\n",
    "\"We can see the influence of science everywhere.\",\n",
    "\"Science is a constant companion of our life.\",\n",
    "\"We have made the impossible things possible with the help of science.\",\n",
    "\"Modern civilization is a contribution of science.\",\n",
    "\"Science should be devoted to the greater welfare of mankind.\",\n",
    "\"Rabindranath Tagore got the Nobel Prize in 1913 which is 98 years ago from today.\",\n",
    "\"He was awarded this prize for the translation of the Bengali 'Gitanjali' into English.\",\n",
    "\"This excellent rendering was the poet's own.\",\n",
    "\"In the English version of Gitanjali there are 103 songs.\"]\n",
    "\n",
    "dataset = MyDataset(text, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30]) tensor([[ 3],\n",
      "        [ 5],\n",
      "        [62],\n",
      "        [68],\n",
      "        [95]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  5, 62, 68, 95])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('wildlife')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b10d533ec9c3c114c7e3be0b60027fd8c2557fa894369e5e67088e5e66ae752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
